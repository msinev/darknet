[net]
subdivisions=1
inputs=6
batch = 10
momentum=0.4
decay=0.0002
max_batches = 20000000
max_time = 300
time_steps=1
learning_rate=0.1
policy=steps
steps=1000,1500
scales=.1,.1


# activation:    logistic,loggy,relu,elu,selu,relie
#       plse       hardtan       lhtan       linear       ramp       leaky       tanh       stair

[connected]
output=50
activation=tanh

[connected]
output=50
activation=tanh

#[dropout]
#probability=.5

#[connected]
#output=32
#activation=leaky


#[connected]
#output=64
#activation=relu


#[dropout]
#probability=.5

[connected]
output=20
activation=relu

#[softmax]
#groups=1

# cost
# types:  sse (sum of squared errors)
#         smooth (sse for [0-1] linear above 1 )
#         masked  sse but ignore some neurons where target set to -1234 aka SECRET_NUM
[cost]
type=sse

